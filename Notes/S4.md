# [Efficiently Modeling Long Sequences with Structured State Spaces](https://arxiv.org/abs/2111.00396)

To solve the long-range dependencies (sequence with 16k+ tokens), RNNs, CNNs, and Transformers all have their own modified version. But authors think state space model (SSM) is 
better for this type of problem. However, SSM needs more computing resources, memory, and is not numerical stable. 

The continuous version of SSM is:
<img width="514" height="160" alt="image" src="https://github.com/user-attachments/assets/0c5eb901-4b5f-43ad-8ec5-5a91905887b2" />

The discrete version is (D=0, no skip connection):
<img width="1276" height="164" alt="image" src="https://github.com/user-attachments/assets/df724dcc-9468-4976-8908-76f32ab1811d" />

So the discrete version can be written into a convolutional version. And the main point is effectively calculate kernel K.
<img width="1896" height="778" alt="image" src="https://github.com/user-attachments/assets/95518b34-ddc7-4420-8035-ea269ed0ce16" />

After some proofs, the A can be written as a normal part plus a low-rank part. 
<img width="1918" height="346" alt="image" src="https://github.com/user-attachments/assets/d2dd26fa-c1ca-426a-9bed-00c8ca6a7f1f" />

And after some proofs, the Kernel K with length of L can be calculated in spectrum space first and iFFT to time space, which will be faster. 
<img width="1892" height="580" alt="image" src="https://github.com/user-attachments/assets/d4dc0042-c766-4f3e-92fa-71c4d5b2570c" />

Î›, P, Q, B, and C are all learnable parameters.
